import torch
import re
from collections import defaultdict
import os
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from .tensor_helper import TensorHelper, TensorConfig
from verl.protocol import DataProto
from verl.utils.tracking import Tracking
import shutil
import requests
from verl.protocol import DataProto, pad_dataproto_to_divisor, unpad_dataproto
import numpy as np
from .gpt_search import call_gpt
from image_store import *
from search_store import extract_search_results
from tensordict import TensorDict
@dataclass
class GenerationConfig:
    max_turns: int
    max_start_length: int
    max_prompt_length: int 
    max_response_length: int
    max_obs_length: int
    num_gpus: int
    no_think_rl: bool=False
    search_url: str = None
    topk: int = 3

class LLMGenerationManager:
    def __init__(
        self,
        tokenizer,
        actor_rollout_ref_wg,
        config: GenerationConfig,
        is_validation: bool = False,
    ):
        self.tokenizer = tokenizer
        self.actor_rollout_ref_wg = actor_rollout_ref_wg
        self.config = config
        self.is_validation = is_validation
        self.tensor_fn = TensorHelper(TensorConfig(
            pad_token_id=tokenizer.pad_token_id,
            max_prompt_length=config.max_prompt_length,
            max_obs_length=config.max_obs_length,
            max_start_length=config.max_start_length
        ))

    def _batch_tokenize(self, responses: List[str]) -> torch.Tensor:
        """Tokenize a batch of responses."""
        return self.tokenizer(
            responses, 
            add_special_tokens=False, 
            return_tensors='pt', 
            padding="longest"
        )['input_ids']

    def _postprocess_responses(self, responses: torch.Tensor) -> torch.Tensor:
        """Process responses to stop at search operation or answer operation."""
        responses_str = self.tokenizer.batch_decode(
            responses, 
            skip_special_tokens=True
        )

        responses_str = [resp.split('</search>')[0] + '</search>'
                 if '</search>' in resp 
                 else resp.split('</answer>')[0] + '</answer>'
                 if '</answer>' in resp 
                 else resp
                 for resp in responses_str]

        if self.config.no_think_rl:
            raise ValueError('stop')
            # if no_think_rl is enabled, only keep action in the str
            actions, _ = self.env.postprocess_predictions(responses_str)
            responses_str=[f"<answer>{envs[idx].ACTION_LOOKUP[action]}</answer>" for idx, action in enumerate(actions)]
            print("RESPONSES:", responses_str)
        responses = self._batch_tokenize(responses_str)
        return responses, responses_str

    def _postprocess_responses1(self, responses: torch.Tensor) -> torch.Tensor:
        """Process responses to stop at search operation or answer operation."""
        responses_str = self.tokenizer.batch_decode(
            responses, 
            skip_special_tokens=False
        )

        if self.config.no_think_rl:
            raise ValueError('stop')
            # if no_think_rl is enabled, only keep action in the str
            actions, _ = self.env.postprocess_predictions(responses_str)
            responses_str=[f"<answer>{envs[idx].ACTION_LOOKUP[action]}</answer>" for idx, action in enumerate(actions)]
            print("RESPONSES:", responses_str)
        responses = self._batch_tokenize(responses_str)
        return responses, responses_str

    def _process_next_obs(self, next_obs: List[str]) -> torch.Tensor:
        """Process next observations from environment."""
        next_obs_ids = self.tokenizer(
            next_obs, 
            padding='longest',
            return_tensors='pt',
            add_special_tokens=False,  # Prevents adding special tokens
        )['input_ids']

        if next_obs_ids.shape[1] > self.config.max_obs_length:
            print(f"[WARNING] OBSERVATION TOO LONG, CONSIDER CHANGING YOUR CONFIG, {next_obs_ids.shape[1]} & {self.config.max_obs_length}")            
            next_obs_ids = next_obs_ids[:, :self.config.max_obs_length]

        return next_obs_ids

    def _update_rolling_state(self, rollings: DataProto, cur_responses: torch.Tensor, 
                            next_obs_ids: torch.Tensor) -> Dict:
        """Update rolling state with new responses and observations."""
        # Concatenate and handle padding        
        new_input_ids = self.tensor_fn.concatenate_with_padding([
            rollings.batch['input_ids'],
            cur_responses,
            next_obs_ids
        ])
        # Create attention mask and position ids
        new_attention_mask = self.tensor_fn.create_attention_mask(new_input_ids)
        new_position_ids = self.tensor_fn.create_position_ids(new_attention_mask)
        # Cut to appropriate length
        effective_len = new_attention_mask.sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)

        attention_mask1 = self.tensor_fn.create_attention_mask(cur_responses)
        effective_len1 = attention_mask1.sum(dim=1).max()
        max_len1 = min(self.config.max_prompt_length, effective_len1)
        attention_mask2 = self.tensor_fn.create_attention_mask(next_obs_ids)
        effective_len2 = attention_mask2.sum(dim=1).max()
        max_len2 = min(self.config.max_prompt_length, effective_len2)

        new_raw_prompt_ids = self.tensor_fn.concatenate_with_padding1([
            rollings.non_tensor_batch['raw_prompt_ids'],
            cur_responses[:, -max_len1:],
            next_obs_ids[:, -max_len2:]
        ])

        new_rollings = DataProto.from_dict(
            tensors={
            'input_ids': new_input_ids[:, -max_len:],
            'position_ids': new_position_ids[:, -max_len:],
            'attention_mask': new_attention_mask[:, -max_len:]
        },
            non_tensors={
            'raw_prompt_ids':new_raw_prompt_ids,
            'multi_modal_data':rollings.non_tensor_batch['multi_modal_data']
        }
            )
        new_rollings.meta_info.update(rollings.meta_info)
        
        return new_rollings
    def _update_rolling_state1(self, rollings: DataProto,  
                            next_obs_ids: torch.Tensor) -> Dict:
        """Update rolling state with new responses and observations."""
        # Concatenate and handle padding        
        new_input_ids = self.tensor_fn.concatenate_with_padding([
            rollings.batch['input_ids'],
            next_obs_ids
        ])

        # Create attention mask and position ids
        new_attention_mask = self.tensor_fn.create_attention_mask(new_input_ids)
        new_position_ids = self.tensor_fn.create_position_ids(new_attention_mask)

        # Cut to appropriate length
        effective_len = new_attention_mask.sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)
        attention_mask1 = self.tensor_fn.create_attention_mask(next_obs_ids)
        effective_len1 = attention_mask1.sum(dim=1).max()
        max_len1 = min(self.config.max_prompt_length, effective_len1)

        new_raw_prompt_ids = self.tensor_fn.concatenate_with_padding1([
            rollings.non_tensor_batch['raw_prompt_ids'],
            next_obs_ids[:, -max_len1:],
        ])

        new_rollings = DataProto.from_dict(
            tensors={
            'input_ids': new_input_ids[:, -max_len:],
            'position_ids': new_position_ids[:, -max_len:],
            'attention_mask': new_attention_mask[:, -max_len:]
        },
            non_tensors={
            'raw_prompt_ids':new_raw_prompt_ids,
            'multi_modal_data':rollings.non_tensor_batch['multi_modal_data']
        }
            )
        new_rollings.meta_info.update(rollings.meta_info)
        
        return new_rollings
    def _info_masked_concatenate_with_padding(self, 
                prompt: torch.Tensor, 
                prompt_with_mask: torch.Tensor, 
                response: torch.Tensor, 
                info: torch.Tensor = None,
                pad_to_left: bool = True
            ) -> torch.Tensor:
        """Concatenate tensors and handle padding. Additionally, create a mask (info_mask) to cover the information block if it exists."""
        pad_id = self.tokenizer.pad_token_id
        tensors = [prompt, response]
        tensors_with_mask = [prompt_with_mask, response]
        if info is not None:
            tensors.append(info)
            info_mask = torch.full(info.size(), 1, dtype=info.dtype, device=info.device) # information mask
            tensors_with_mask.append(info_mask)
        
        concatenated = torch.cat(tensors, dim=1)
        concatenated_with_info = torch.cat(tensors_with_mask, dim=1)
        mask = concatenated != pad_id if pad_to_left else concatenated == pad_id
        sorted_indices = mask.to(torch.int64).argsort(dim=1, stable=True)
        padded_tensor = concatenated.gather(1, sorted_indices)
        padded_tensor_with_info = concatenated_with_info.gather(1, sorted_indices)
        padded_tensor_with_info = torch.ones_like(padded_tensor_with_info)

        return padded_tensor, padded_tensor_with_info

    def _update_right_side(self, right_side: Dict, 
                          cur_responses: torch.Tensor,
                          next_obs_ids: torch.Tensor = None) -> Dict:
        """Update right side state."""
        if next_obs_ids != None:
            responses, response_mask = self._info_masked_concatenate_with_padding(
                    right_side['responses'],
                    right_side['response_mask'],
                    cur_responses,
                    next_obs_ids, 
                    pad_to_left=False
                )
        else:
            responses, response_mask = self._info_masked_concatenate_with_padding(
                    right_side['responses'],
                    right_side['response_mask'],
                    cur_responses,
                    pad_to_left=False
                )
        effective_len = self.tensor_fn.create_attention_mask(responses).sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)
        
        return {'responses': responses[:, :max_len], 'response_mask': response_mask[:, :max_len]}


    def _generate_with_gpu_padding(self, active_batch: DataProto) -> DataProto:
        """
            Wrapper for generation that handles multi-GPU padding requirements.
            if num_gpus <= 1, return self.actor_rollout_ref_wg.generate_sequences(active_batch)
            if active_batch size is not divisible by num_gpus, pad with first sequence
            then remove padding from output
        """
        num_gpus = self.config.num_gpus
        if num_gpus <= 1:
            return self.actor_rollout_ref_wg.generate_sequences(active_batch)
            
        batch_size = active_batch.batch['input_ids'].shape[0]
        remainder = batch_size % num_gpus
        
        for key in active_batch.batch.keys():
            active_batch.batch[key] = active_batch.batch[key].long()
        if remainder == 0:
            return self.actor_rollout_ref_wg.generate_sequences(active_batch)
        
        # Add padding sequences
        padding_size = num_gpus - remainder
        padded_batch = {}
        padded_non_tensor_batch={}
        for k, v in active_batch.batch.items():
            # Use first sequence as padding template
            pad_sequence = v[0:1].repeat(padding_size, *[1] * (len(v.shape) - 1))
            padded_batch[k] = torch.cat([v, pad_sequence], dim=0)

        for k, v in active_batch.non_tensor_batch.items():
            # 使用第一个元素作为填充模板
            pad_sequence = np.repeat(v[0:1], padding_size, axis=0)
            padded_non_tensor_batch[k] = np.concatenate([v, pad_sequence], axis=0)


        padded_active_batch = DataProto.from_dict(padded_batch,padded_non_tensor_batch,active_batch.meta_info)
        for key in padded_active_batch.batch.keys():
            padded_active_batch.batch[key] = padded_active_batch.batch[key].long()

        # Generate with padded batch
        padded_output = self.actor_rollout_ref_wg.generate_sequences(padded_active_batch)

        # Remove padding from output
        trimmed_batch = {k: v[:-padding_size] for k, v in padded_output.batch.items()}
        
        # Handle meta_info if present
        if hasattr(padded_output, 'meta_info') and padded_output.meta_info:
            trimmed_meta = {}
            for k, v in padded_output.meta_info.items():
                if isinstance(v, torch.Tensor):
                    trimmed_meta[k] = v[:-padding_size]
                else:
                    trimmed_meta[k] = v
            padded_output.meta_info = trimmed_meta
            
        padded_output.batch = trimmed_batch
        return padded_output           

    def _generate_with_gpu_padding1(self, active_batch: DataProto) -> DataProto:
        """
            Wrapper for generation that handles multi-GPU padding requirements.
            if num_gpus <= 1, return self.actor_rollout_ref_wg.generate_sequences1(active_batch)
            if active_batch size is not divisible by num_gpus, pad with first sequence
            then remove padding from output
        """
        num_gpus = self.config.num_gpus
        if num_gpus <= 1:
            return self.actor_rollout_ref_wg.generate_sequences1(active_batch)
            
        batch_size = active_batch.batch['input_ids'].shape[0]
        remainder = batch_size % num_gpus
        
        for key in active_batch.batch.keys():
            active_batch.batch[key] = active_batch.batch[key].long()
        if remainder == 0:
            return self.actor_rollout_ref_wg.generate_sequences1(active_batch)
        
        # Add padding sequences
        padding_size = num_gpus - remainder
        padded_batch = {}
        padded_non_tensor_batch={}
        for k, v in active_batch.batch.items():
            # Use first sequence as padding template
            pad_sequence = v[0:1].repeat(padding_size, *[1] * (len(v.shape) - 1))
            padded_batch[k] = torch.cat([v, pad_sequence], dim=0)

        for k, v in active_batch.non_tensor_batch.items():
            # 使用第一个元素作为填充模板
            pad_sequence = np.repeat(v[0:1], padding_size, axis=0)
            padded_non_tensor_batch[k] = np.concatenate([v, pad_sequence], axis=0)


        padded_active_batch = DataProto.from_dict(padded_batch,padded_non_tensor_batch,active_batch.meta_info)
        for key in padded_active_batch.batch.keys():
            padded_active_batch.batch[key] = padded_active_batch.batch[key].long()

        # Generate with padded batch
        padded_output = self.actor_rollout_ref_wg.generate_sequences1(padded_active_batch)

        # Remove padding from output
        trimmed_batch = {k: v[:-padding_size] for k, v in padded_output.batch.items()}
        
        # Handle meta_info if present
        if hasattr(padded_output, 'meta_info') and padded_output.meta_info:
            trimmed_meta = {}
            for k, v in padded_output.meta_info.items():
                if isinstance(v, torch.Tensor):
                    trimmed_meta[k] = v[:-padding_size]
                else:
                    trimmed_meta[k] = v
            padded_output.meta_info = trimmed_meta
            
        padded_output.batch = trimmed_batch
        return padded_output           

    def _update_active_mask(self,active_mask, curr_active_mask):
        """
        更新 result 张量，根据 curr_active_mask 和 active_mask 的值。

        参数:
        - curr_active_mask (torch.Tensor): 当前激活掩码，布尔型张量。
        - active_mask (torch.Tensor): 激活掩码，布尔型张量。

        返回:
        - result (torch.Tensor): 更新后的结果张量，布尔型张量。
        """
        # 获取 active_mask 中值为 True 的索引
        true_indices = torch.nonzero(active_mask, as_tuple=True)[0]
        
        # 创建一个与 active_mask 形状相同且所有值为 False 的张量
        result = torch.full_like(active_mask, False)
        
        # 遍历 true_indices，将 curr_active_mask 中对应的值赋给 result 中的对应索引位置
        i = 0
        for idx in true_indices:
            result[idx] = curr_active_mask[i]
            i += 1
        
        return result
        
    def run_llm_loop(self, gen_batch, initial_input_ids: torch.Tensor) -> Tuple[Dict, Dict]:
        """Run main LLM generation loop."""
        original_left_side = {'input_ids': initial_input_ids[:, -self.config.max_start_length:]}
        original_right_side = {'responses': initial_input_ids[:, []], 'response_mask': initial_input_ids[:, []]}
        active_mask = torch.ones(gen_batch.batch['input_ids'].shape[0], dtype=torch.bool)
        turns_stats = torch.ones(gen_batch.batch['input_ids'].shape[0], dtype=torch.int)
        valid_action_stats = torch.zeros(gen_batch.batch['input_ids'].shape[0], dtype=torch.int)
        valid_search_stats = torch.zeros(gen_batch.batch['input_ids'].shape[0], dtype=torch.int)
        active_num_list = [active_mask.sum().item()]
        rollings = gen_batch
        non_tensor_batch= {"multi_modal_data": gen_batch.non_tensor_batch['multi_modal_data']}   
        for step in range(self.config.max_turns):
            if active_mask.sum():
                    # break
                rollings.batch = self.tensor_fn.cut_to_effective_len(
                    rollings.batch,
                    batch_keys=["input_ids", "attention_mask", "position_ids"]
                )

                # 构造新的 DataProto 对象
                rollings_active = DataProto.from_dict(
                    tensors={k: v[active_mask] for k, v in rollings.batch.items() if isinstance(v, torch.Tensor)},
                    non_tensors={k: [item for item, mask in zip(v, active_mask) if mask] for k, v in rollings.non_tensor_batch.items()},
                    meta_info=rollings.meta_info
                )
                # breakpoint()
                gen_output = self._generate_with_gpu_padding1(rollings_active)
                meta_info = gen_output.meta_info     
                responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
                
                # print(responses_str)
                responses_ids, responses_str = self.tensor_fn._example_level_pad(responses_ids, responses_str, active_mask)
                # break point()
                # Execute in environment and process observations
                
                next_obs, dones, valid_action, is_search = self.execute_predictions(
                    responses_str,gen_batch.non_tensor_batch["multi_modal_data"], self.tokenizer.pad_token,active_mask
                )
                
                curr_active_mask = torch.tensor([not done for done in dones], dtype=torch.bool)
                print("curr_active_mask:", curr_active_mask)
                print("active_mask:", active_mask)
                # curr_active_mask=self._update_active_mask(active_mask,curr_active_mask)
                active_mask = active_mask * curr_active_mask
                active_num_list.append(active_mask.sum().item())
                turns_stats[curr_active_mask] += 1


                valid_action_stats += torch.tensor(valid_action, dtype=torch.int)
                valid_search_stats += torch.tensor(is_search, dtype=torch.int)
                next_obs_ids = self._process_next_obs(next_obs)
                # Update states

                rollings = self._update_rolling_state(
                    rollings,
                    responses_ids,
                    next_obs_ids
                )
                original_right_side = self._update_right_side(
                    original_right_side,
                    responses_ids,
                    # next_obs_ids
                )

        # final LLM rollout
        if active_mask.sum():
            batch_size = gen_batch.batch['input_ids'].size(0)
            next_obs="<think>Based on the above information, I should directly give the final answer.</think>"
            expanded_next_obs = [next_obs] * batch_size
            next_obs_ids = self._process_next_obs(expanded_next_obs)
            rollings = self._update_rolling_state1(
                        rollings,
                        next_obs_ids
                    )
            rollings.batch = self.tensor_fn.cut_to_effective_len(
                rollings.batch,
                batch_keys=["input_ids", "attention_mask", "position_ids"]
            )
                
            rollings_active = DataProto.from_dict(
                tensors={k: v[active_mask] for k, v in rollings.batch.items() if isinstance(v, torch.Tensor)},
                non_tensors={k: [item for item, mask in zip(v, active_mask) if mask] for k, v in rollings.non_tensor_batch.items()},
                meta_info=rollings.meta_info            
            )

            gen_output = self._generate_with_gpu_padding1(rollings_active)

            meta_info = gen_output.meta_info     
            responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
            responses_ids, responses_str = self.tensor_fn._example_level_pad(responses_ids, responses_str,active_mask)
            print(responses_str)
            # # Execute in environment and process observations
            _, dones, valid_action, is_search = self.execute_predictions(
                responses_str,gen_batch.non_tensor_batch["multi_modal_data"], self.tokenizer.pad_token,active_mask,do_search=False
            )

            curr_active_mask = torch.tensor([not done for done in dones], dtype=torch.bool)
            active_mask = active_mask * curr_active_mask
            active_num_list.append(active_mask.sum().item())
            valid_action_stats += torch.tensor(valid_action, dtype=torch.int)
            valid_search_stats += torch.tensor(is_search, dtype=torch.int)
            # breakpoint()
            original_right_side = self._update_right_side(
                original_right_side,
                responses_ids,
            )

        meta_info['turns_stats'] = turns_stats.tolist()
        meta_info['active_mask'] = active_mask.tolist()
        meta_info['valid_action_stats'] = valid_action_stats.tolist()
        meta_info['valid_search_stats'] = valid_search_stats.tolist()
        gen_output.meta_info.update(meta_info)
        print("ACTIVE_TRAJ_NUM:", active_num_list)

        return self._compose_final_output(original_left_side, original_right_side,meta_info,non_tensor_batch)

    def _compose_final_output(self, left_side: Dict,
                            right_side: Dict,
                            meta_info: Dict,non_tensor_batch) -> Tuple[Dict, Dict]:
        """Compose final generation output."""
        final_output = right_side.copy()
        final_output['prompts'] = left_side['input_ids']
        # breakpoint()
        # Combine input IDs
        final_output['input_ids'] = torch.cat([
            left_side['input_ids'],
            right_side['responses']
        ], dim=1)
        
        # Create attention mask and position ids
        final_output['attention_mask'] = torch.cat([
            self.tensor_fn.create_attention_mask(left_side['input_ids']),
            self.tensor_fn.create_attention_mask(final_output['responses'])
        ], dim=1)
        final_output['info_mask'] = torch.cat([
            self.tensor_fn.create_attention_mask(left_side['input_ids']),
            self.tensor_fn.create_attention_mask(final_output['response_mask'])
        ], dim=1)
        
        final_output['position_ids'] = self.tensor_fn.create_position_ids(
            final_output['attention_mask']
        )
        final_output = TensorDict(final_output, batch_size=final_output['input_ids'].shape[0])
        final_output = DataProto(batch=final_output,non_tensor_batch=non_tensor_batch,meta_info=meta_info)
        # final_output.meta_info.update(meta_info)

        
        return final_output

    def execute_predictions(self,predictions:List[str],image_path: np.ndarray,pad_token:str, active_mask=None,do_search=True,) -> List[str]:
        """
        Execute predictions across multiple environments.
        NOTE: the function is the actual `step` function in the environment
        NOTE penalty_for_invalid is not included in observation shown to the LLM
        
        Args:
            envs: List of environment instances
            predictions: List of action predictions
            pad_token: Token to use for padding
            
        Returns:
            List of observation strings
        """
        # image_paths = [pred['images'][0]['path'] for pred in image_path]
        next_obs, dones, valid_action, is_search = [], [], [], []

        cur_actions, contents = self.postprocess_predictions(predictions)
        image_paths = [pred['images'][0] for pred in image_path]
        tool_results1=''
        tool_results2=''
        tool_results = []
        search_results = []
        # search_queries = [content for action, content in zip(cur_actions, contents) if action == 'search']
        # tool_queries = [content for action, content in zip(cur_actions, contents) if action == 'tool']
        if do_search:
            for i, (action, content) in enumerate(zip(cur_actions, contents)):
                if action == 'tool':
                    if content == "image_detection":
                        # tool_results1 = manipulation_detection(image_paths[i])
                        # tool_results1 +="、" +diffusion_generated_detection(image_paths[i])
                        # breakpoint()
                        tool_results1=extract_detection_results(image_paths[i])
                        tool_results.append(tool_results1)
                        print(tool_results1)
                    else:
                        previous_action = "My previous action is invalid. Let me try again."
                        tool_results.append(f'\n{previous_action}\n')
                if action == 'search':
                    # prompt = "please find something about {entity} and answer briefly."
                    # prompt = prompt.format(entity=content)
                    # knowledge = call_gpt(prompt)
                    search_results1=extract_search_results(image_paths[i])
                    # search_results.append(knowledge)
                    print("search")
                    search_results.append(search_results1)
                # search_results = self.batch_search(search_queries)
            assert len(search_results) == sum([1 for action in cur_actions if action == 'search'])
        else:
            search_results = [''] * sum([1 for action in cur_actions if action == 'search'])
            tool_results = [''] * sum([1 for action in cur_actions if action == 'tool'])
        
        for i, (action, active) in enumerate(zip(cur_actions, active_mask)):
            if not active:
                next_obs.append('')
                dones.append(1)
                valid_action.append(0)
                is_search.append(0)
            else:
                if action == 'answer':
                    next_obs.append('')
                    dones.append(1)
                    valid_action.append(1)
                    is_search.append(0)
                elif action == 'search':
                    # through an internal monologue,enclosed within <think> </think> tags.<think>{previous_action}</think>
                    # previous_action="Based on the searched results, go on reasoning and determine whether the news caption exhibits textual veracity distortion. If not, proceed to assess other types of misinformation."
                    # next_obs.append(f'\n\n<information>{search_results.pop(0).strip()}</information>{previous_action}\n\n')
                    next_obs.append(f'\n\n<information>{search_results.pop(0).strip()}</information>\n\n')
                    dones.append(0)
                    valid_action.append(1)
                    is_search.append(1)
                elif action == 'tool':
                    next_obs.append(f'\n\n<information>{tool_results.pop(0).strip()}</information>\n\n')
                    dones.append(0)
                    valid_action.append(1)
                    is_search.append(1)
                else:
                    previous_action = "My previous action is invalid.I should go on reasoning."
                    next_obs.append(f'\n<think>{previous_action}</think>\n')
                    # next_obs.append('')
                    dones.append(0)
                    valid_action.append(0)
                    is_search.append(0)
            
        # assert len(search_results) == 0


        return next_obs, dones, valid_action, is_search

    def postprocess_predictions(self, predictions: List[Any]) -> Tuple[List[int], List[bool]]:
        """
        Process (text-based) predictions from llm into actions and validity flags.
        
        Args:
            predictions: List of raw predictions
            
        Returns:
            Tuple of (actions list, validity flags list)
        """
        actions = []
        contents = []
        for prediction in predictions:
            if isinstance(prediction, str): # for llm output
                pattern = r'<(search|tool|answer)>(.*?)</\1>'
                match = re.search(pattern, prediction, re.DOTALL)
                # match = re.findall(pattern, prediction, re.DOTALL)
                if match:
                    content = match.group(2).strip()  # Return only the content inside the tags
                    action = match.group(1)
                    # last_match = match[-1]
                    # action = last_match[0]
                    # content = last_match[1].strip()
                else:
                    content = ''
                    action = None

            else:
                raise ValueError(f"Invalid prediction type: {type(prediction)}")
            
            actions.append(action)
            contents.append(content)
            
        return actions, contents

    def batch_search(self, queries: List[str] = None) -> str:
        """
        Batchified search for queries.
        Args:
            queries: queries to call the search engine
        Returns:
            search results which is concatenated into a string
        """
        results = self._batch_search(queries)['result']
        
        return [self._passages2string(result) for result in results]

    def _batch_search(self, queries):
        
        payload = {
            "queries": queries,
            "topk": self.config.topk,
            "return_scores": True
        }
        print("Payload:", payload)
        # return requests.post(self.config.search_url, json=payload).json()
        try:
            response = requests.post(self.config.search_url, json=payload)
            if response.status_code != 200:
                raise Exception(f"Request failed with status code {response.status_code}: {response.text}")
            if not response.text:
                raise Exception("Response content is empty")
            results = response.json()
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            raise
        return results

    def _passages2string(self, retrieval_result):
        format_reference = ''
        for idx, doc_item in enumerate(retrieval_result):
            
            content = doc_item['document']['contents']
            title = content.split("\n")[0]
            text = "\n".join(content.split("\n")[1:])
            format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"

        return format_reference



